{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb1b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der Bibliotheken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import contextlib\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d69aa0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensätze laden\n",
    "daten1950=pd.read_csv(\"daten1950.csv\").drop([\"Unnamed: 0\"], axis=1)[:-1]\n",
    "daten1973=pd.read_csv(\"daten1973.csv\").drop([\"Unnamed: 0\"], axis=1)[:-1]\n",
    "daten1990=pd.read_csv(\"daten1990.csv\").drop([\"Unnamed: 0\"], axis=1)[:-1]\n",
    "daten2006=pd.read_csv(\"daten2006.csv\").drop([\"Unnamed: 0\"], axis=1)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7feae74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion der Prognosemodelle\n",
    "# daten: daten1950, daten1973, daten1990, daten2006 - Welcher Datensatz genutzt wird. \n",
    "# methode: \"lr\", \"pr\", \"tree\", \"nn\", \"lstm\", \"sv\", \"knn\", \"arima\", \"arimax\" - Das zu nutzende Modell\n",
    "# ensemble: \"bagging\", \"ada_boosting\" - Nutzung der Ensemble-Algorithmen\n",
    "# x/y test/train: Trainings- und Testdaten. Werden über die Funktion compare_result() automatisch übergeben\n",
    "def modell(methode, ensemble, x_train, x_test, y_train, y_test, daten):\n",
    "    y_pred = None  # Vorab-Initialisierung von y_pred\n",
    "#     ensemble = None\n",
    "\n",
    "    # Lineare Regression\n",
    "    if methode == \"lr\":\n",
    "        model = LinearRegression()\n",
    "\n",
    "    # Polynom Regression\n",
    "    elif methode == \"pr\":\n",
    "        # Polynomiale Merkmale erstellen\n",
    "        if daten.equals(daten1950):\n",
    "            poly_features = PolynomialFeatures(degree=2)\n",
    "        elif daten.equals(daten1973):\n",
    "            poly_features = PolynomialFeatures(degree=4)\n",
    "        elif daten.equals(daten1990):\n",
    "            poly_features = PolynomialFeatures(degree=2)\n",
    "        elif daten.equals(daten2006):\n",
    "            poly_features = PolynomialFeatures(degree=2)\n",
    "        else: \n",
    "            pass\n",
    "        \n",
    "        x_train = poly_features.fit_transform(x_train)\n",
    "        x_test = poly_features.transform(x_test)\n",
    "\n",
    "        # Modell erstellen und trainieren\n",
    "        model = LinearRegression()\n",
    "        \n",
    "    # Entscheidungsbaum\n",
    "    elif methode == \"tree\":\n",
    "        model = DecisionTreeRegressor(random_state=40)\n",
    "\n",
    " # Neuronales Netzwerk\n",
    "    elif methode == \"nn\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(124, activation='relu', input_shape=(x_train.shape[1],)),\n",
    "            tf.keras.layers.Dense(124, activation='relu'),\n",
    "            tf.keras.layers.Dense(124, activation='relu'),\n",
    "            tf.keras.layers.Dense(62, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)  # Ausgabeschicht\n",
    "        ])\n",
    "\n",
    "        # Modell kompilieren\n",
    "        model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "        \n",
    "        # Modell trainieren\n",
    "        model.fit(x_train, y_train, epochs=200, batch_size=32, verbose=False)\n",
    "\n",
    "        y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "    # LSTM\n",
    "    elif methode == \"lstm\":\n",
    "        # Umwandeln der Daten in das erforderliche 3D-Format für LSTM\n",
    "        x_train_lstm = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "        x_test_lstm = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "        # Modell erstellen\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(124, activation='relu', return_sequences=True, input_shape=(1, x_train.shape[1])))\n",
    "        model.add(LSTM(124, activation='relu', return_sequences=True))\n",
    "        model.add(LSTM(124, activation='relu', return_sequences=True))\n",
    "        model.add(LSTM(62, activation='relu', return_sequences=True))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        # Modell kompilieren\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "            \n",
    "        # Modell trainieren\n",
    "        model.fit(x_train_lstm, y_train, epochs=200, batch_size=32, verbose=False)\n",
    "\n",
    "        # Anpassen für die Weitere Analyse\n",
    "        x_test = x_test_lstm\n",
    "        x_train = x_train_lstm\n",
    "\n",
    "        y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "\n",
    "    # Support Vector Regression\n",
    "    elif methode == \"sv\":\n",
    "        # Modellparameter festlegen\n",
    "        if daten.equals(daten1950):\n",
    "            model = SVR(kernel='linear', C=300, epsilon=1)\n",
    "        elif daten.equals(daten1973):\n",
    "            model = SVR(kernel='linear', C=300, epsilon=0.1)\n",
    "        elif daten.equals(daten1990):\n",
    "            model = SVR(kernel='linear', C=300, epsilon=1)\n",
    "        elif daten.equals(daten2006):\n",
    "            model = SVR(kernel='linear', C=300, epsilon=1)\n",
    "            \n",
    "    # KNN-Regression\n",
    "    elif methode == \"knn\":\n",
    "        # Modell initialisieren und trainieren\n",
    "        model = KNeighborsRegressor(n_neighbors=1, p=1, weights=\"distance\")\n",
    "\n",
    "    # Arima\n",
    "    elif methode == \"arima\":\n",
    "        # ARIMA-Modell erstellen und anpassen\n",
    "        if daten.equals(daten1950):\n",
    "            model = ARIMA(y_train, order=(3, 1, 2))\n",
    "        elif daten.equals(daten1973):\n",
    "            model = ARIMA(y_train, order=(7, 1, 1))\n",
    "        elif daten.equals(daten1990):\n",
    "            model = ARIMA(y_train, order=(2, 1, 1))\n",
    "        elif daten.equals(daten2006):\n",
    "            model = ARIMA(y_train, order=(2, 1, 5))\n",
    "        else:\n",
    "            pass\n",
    "        model_fit = model.fit()\n",
    "        y_pred = model_fit.forecast(steps=4)\n",
    "\n",
    "    # Arimax\n",
    "    elif methode == \"arimax\":\n",
    "        # ARIMAX-Modell erstellen und anpassen\n",
    "        if daten.equals(daten1950):\n",
    "            model = ARIMA(y_train, order=(10, 4, 1), exog=x_train)\n",
    "        elif daten.equals(daten1973):\n",
    "            model = ARIMA(y_train, order=(3, 3, 1), exog=x_train)\n",
    "        elif daten.equals(daten1990):\n",
    "            model = ARIMA(y_train, order=(1, 1, 2), exog=x_train)\n",
    "        elif daten.equals(daten2006):\n",
    "            model = ARIMA(y_train, order=(2, 1, 1), exog=x_train)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        model_fit = model.fit()\n",
    "        y_pred = model_fit.forecast(steps=4, exog=x_test)\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Bagging einfügen\n",
    "    if ensemble == \"bagging\":\n",
    "        model = BaggingRegressor(model, random_state=42)\n",
    "        # Modell trainieren\n",
    "        model.fit(x_train, y_train)\n",
    "        # Vorhersagen für Testdaten\n",
    "        y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "    # ADA Boosting einfügen\n",
    "    elif ensemble == \"ada_boosting\":\n",
    "        boosting_model = AdaBoostRegressor(base_estimator=model, random_state=42)\n",
    "        # Modell trainieren\n",
    "        boosting_model.fit(x_train, y_train)\n",
    "        # Vorhersagen für Testdaten\n",
    "        y_pred = boosting_model.predict(x_test).flatten()\n",
    "\n",
    "    # Modellprognosen ohne Bagging, Boosting oder ADA Boosting\n",
    "    elif ensemble != \"bagging\" and ensemble != \"ada_boosting\" and methode != \"nn\" and methode != \"lstm\" and methode != \"arima\" and methode != \"arimax\":\n",
    "        # Modell trainieren\n",
    "        model.fit(x_train, y_train)\n",
    "        # Vorhersagen für Testdaten\n",
    "        y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "# Komponentenanzahl für PCA finden & speichern\n",
    "# daten: daten1950, daten1973, daten1990, daten2006 - Welcher Datensatz genutzt wird. \n",
    "# pca_ausgaben: 1 ist die Ausgabe der Schleife mit verschiedener anzahl an Hauptkomponenten, 2 ist die Rückgabe der PCA Tabellen zur weiterverarbeitung\n",
    "# methode: \"lr\", \"pr\", \"tree\", \"nn\", \"lstm\", \"sv\", \"knn\", \"arima\", \"arimax\" - Das zu nutzende Modell\n",
    "# ensemble: \"bagging\", \"ada_boosting\" - Nutzung der Ensemble-Algorithmen\n",
    "# split: \"validierung\", \"test\" - Definiert, ob der Validierungs- oder Testzeitraum mit den zugehörigen Daten genutzt werden\n",
    "# normalize: True, False - Ob Daten normalisiert werden\n",
    "def find_best_pca(daten, pca_ausgaben, methode, ensemble, split, normalize=True):\n",
    "    # PCA - Dimensionen der Datensätze reduzieren\n",
    "    x = daten.drop([\"co2\"], axis=1)\n",
    "\n",
    "    if normalize:\n",
    "        x = StandardScaler().fit_transform(x)\n",
    "        x = pd.DataFrame(x, columns=daten.columns.drop(\"co2\"))\n",
    "\n",
    "    # Liste für MAE-Werte\n",
    "    mae_values = []\n",
    "    \n",
    "    if split == \"validierung\": \n",
    "        zeit1=-8\n",
    "        zeit2=-4\n",
    "    elif split == \"test\":\n",
    "        zeit1=-4\n",
    "        zeit2=None\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Iteration über verschiedene Anzahlen von PCA-Komponenten\n",
    "    for n in range(2, 11):\n",
    "        # PCA durchführen\n",
    "        pca = PCA(n_components=n)\n",
    "        PC = pca.fit_transform(x)\n",
    "        principalDF = pd.DataFrame(data=PC, columns=[f'pc{i}' for i in range(1, n + 1)])\n",
    "        pca_df = pd.concat([principalDF, daten[['co2']]], axis=1)\n",
    "        \n",
    "        # Festlegen des Trainings- und Testdatenbereichs\n",
    "        x_train = pca_df.drop([\"co2\"], axis=1)[:zeit1]\n",
    "        x_test = pca_df.drop([\"co2\"], axis=1)[zeit1:zeit2]\n",
    "        y_train = pca_df['co2'][:zeit1]\n",
    "        y_test = pca_df['co2'][zeit1:zeit2]\n",
    "        \n",
    "\n",
    "        # Ausgewählte Modellprognose\n",
    "        y_pred = modell(methode, ensemble, x_train, x_test, y_train, y_test, daten)\n",
    "\n",
    "        # Berechnung des Mean Absolute Error (MAE)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mae_values.append(mae)\n",
    "\n",
    "    # Ausgabe der MAE-Werte\n",
    "    for n, mae in zip(range(2, 11), mae_values):\n",
    "        print(f\"Anzahl der PCA-Komponenten: {n}, MAE: {mae}\")\n",
    "\n",
    "    # Index des niedrigsten MAE-Werts ermitteln\n",
    "    min_mae_index = np.argmin(mae_values)\n",
    "\n",
    "    # PCA mit niedrigstem MAE\n",
    "    best_pca = PCA(n_components=min_mae_index + 2)\n",
    "    PC = best_pca.fit_transform(x)\n",
    "    principalDF = pd.DataFrame(data=PC, columns=[f'pc{i}' for i in range(1, min_mae_index + 3)])\n",
    "    pca_df = pd.concat([principalDF, daten[['co2']]], axis=1)\n",
    "\n",
    "    # Gibt mit pca_ausgaben==1 die MAE über alle Iterationen wieder und mit pca_ausgaben==2 speichert es die beste PCA für die weitere Analyse\n",
    "    if pca_ausgaben==1:\n",
    "        return print(\"Niedrigster MAE:\", min_mae_index+2)\n",
    "    elif pca_ausgaben==2:\n",
    "        return best_pca, pca_df\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Vergleiche die beste MAE mit den nicht-transformierten PCA Werten / Gebe die Statistiken und Ergebnisse der Prognosen durch\n",
    "# daten: daten1950, daten1973, daten1990, daten2006 - Welcher Datensatz genutzt wird. \n",
    "# methode: \"lr\", \"pr\", \"tree\", \"nn\", \"lstm\", \"sv\", \"knn\", \"arima\", \"arimax\" - Das zu nutzende Modell\n",
    "# ensemble: \"bagging\", \"ada_boosting\" - Nutzung der Ensemble-Algorithmen\n",
    "# normalize: True, False - Ob Daten normalisiert werden\n",
    "# split: \"validierung\", \"test\" - Definiert, ob der Validierungs- oder Testzeitraum mit den zugehörigen Daten genutzt werden\n",
    "def compare_result(daten, methode, ensemble, split, normalize=True):\n",
    "    # PCA mit niedrigstem MSE finden\n",
    "    best_pca, pca_df = find_best_pca(daten, 2, methode, ensemble, split, normalize=normalize)\n",
    "\n",
    "    if split == \"validierung\": \n",
    "        zeit1=-8\n",
    "        zeit2=-4\n",
    "    elif split == \"test\":\n",
    "        zeit1=-4\n",
    "        zeit2=None\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Prognose mit PCA\n",
    "    x_train_pca = pca_df.drop([\"co2\"], axis=1)[:zeit1]\n",
    "    x_test_pca = pca_df.drop([\"co2\"], axis=1)[zeit1:zeit2]\n",
    "    y_train_pca = pca_df['co2'][:zeit1]\n",
    "    y_test_pca = pca_df['co2'][zeit1:zeit2]\n",
    "\n",
    "    # Modell mit PCA trainieren\n",
    "    y_pred_pca = modell(methode, ensemble, x_train_pca, x_test_pca, y_train_pca, y_test_pca, daten)\n",
    "    \n",
    "    # Prognose ohne PCA\n",
    "    x_train = daten.drop([\"co2\"], axis=1)[:zeit1]\n",
    "    x_test = daten.drop([\"co2\"], axis=1)[zeit1:zeit2]\n",
    "    y_train = daten['co2'][:zeit1]\n",
    "    y_test = daten['co2'][zeit1:zeit2]\n",
    "\n",
    "    # Modell mit PCA trainieren\n",
    "    y_pred = modell(methode, ensemble, x_train, x_test, y_train, y_test, daten)\n",
    "\n",
    "    # MAE berechnen\n",
    "    mae_pca = mean_absolute_error(y_test_pca, y_pred_pca)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # Ausgabe der MAE\n",
    "    print(\"MAE mit PCA:\", mae_pca)\n",
    "    print(\"MAE mit PCA:\", mae)\n",
    "\n",
    "    # Neue Daten für einen Vergleich der Daten vorbereiten\n",
    "    neue_tabelle = pd.DataFrame({'PCA y': y_pred_pca, \"Co2 Emissionen\": y_test_pca})\n",
    "    d = {'jahr': daten[\"jahr\"][zeit1:zeit2], 'Co2 Wert': daten[\"co2\"][zeit1:zeit2], \"PCA y\": daten[\"co2\"][zeit1:zeit2]}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df[\"PCA y\"][-4:] = neue_tabelle[\"PCA y\"]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "851ac713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter sind für die Prognosen fixiert worden\n",
    "# Nur die wichtigen Modelle wurden integriert\n",
    "# PCA wird immer durchgeführt\n",
    "\n",
    "\n",
    "# Funktion der Prognosemodelle\n",
    "# daten: daten1950, daten1973, daten1990, daten2006 - Welcher Datensatz genutzt wird\n",
    "# methode: \"lr\", \"pr\", \"sv\", \"knn\", \"arima\", \"arimax\" - Das zu nutzende Modell\n",
    "# x/y test/train: Trainings- und Testdaten. Werden über die Funktion compare_result2() automatisch übergeben\n",
    "def modell2(methode, x_train, x_test, y_train, y_test, daten):\n",
    "    # Vorab-Initialisierungen\n",
    "    y_pred = None\n",
    "    ensemble = None\n",
    "\n",
    "    # Lineare Regression\n",
    "    if methode == \"lr\":\n",
    "        model = LinearRegression()\n",
    "        if daten.equals(daten1950):\n",
    "            ensemble = \"ada_boosting\"\n",
    "        elif daten.equals(daten1973):\n",
    "            poly_features = PolynomialFeatures(degree=2)\n",
    "        elif daten.equals(daten1990):\n",
    "            poly_features = PolynomialFeatures(degree=2)\n",
    "            ensemble = \"bagging\"\n",
    "        elif daten.equals(daten2006):\n",
    "            ensemble = \"bagging\"\n",
    "        else: \n",
    "            pass\n",
    "\n",
    "    # Polynom Regression\n",
    "    elif methode == \"pr\":\n",
    "        # Polynomiale Merkmale erstellen\n",
    "        if daten.equals(daten1950):\n",
    "            poly_features = PolynomialFeatures(degree=4)\n",
    "        elif daten.equals(daten1973):\n",
    "            poly_features = PolynomialFeatures(degree=2)\n",
    "            ensemble = \"bagging\"\n",
    "        elif daten.equals(daten1990):\n",
    "            poly_features = PolynomialFeatures(degree=2)\n",
    "            ensemble = \"ada_boosting\"\n",
    "        elif daten.equals(daten2006):\n",
    "            poly_features = PolynomialFeatures(degree=2)\n",
    "        else: \n",
    "            pass\n",
    "        \n",
    "        x_train = poly_features.fit_transform(x_train)\n",
    "        x_test = poly_features.transform(x_test)\n",
    "\n",
    "        # Modell erstellen und trainieren\n",
    "        model = LinearRegression()\n",
    "        \n",
    "    # Support Vector Regression\n",
    "    elif methode == \"sv\":\n",
    "        # Modellparameter festlegen\n",
    "        if daten.equals(daten1950):\n",
    "            model = SVR(kernel='linear', C=300, epsilon=1)\n",
    "            ensemble = \"ada_boosting\"\n",
    "        elif daten.equals(daten1973):\n",
    "            model = SVR(kernel='linear', C=300, epsilon=0.1)\n",
    "            ensemble = \"ada_boosting\"\n",
    "        elif daten.equals(daten1990):\n",
    "            model = SVR(kernel='linear', C=300, epsilon=1)\n",
    "            ensemble = \"bagging\"\n",
    "        elif daten.equals(daten2006):\n",
    "            model = SVR(kernel='linear', C=300, epsilon=1)\n",
    "            ensemble = \"ada_boosting\"\n",
    "            \n",
    "    # KNN-Regression\n",
    "    elif methode == \"knn\":\n",
    "        # Modell initialisieren und trainieren\n",
    "        model = KNeighborsRegressor(n_neighbors=1, p=1, weights=\"distance\")\n",
    "\n",
    "    # Arima\n",
    "    elif methode == \"arima\":\n",
    "        # ARIMA-Modell erstellen und anpassen\n",
    "        if daten.equals(daten1950):\n",
    "            model = ARIMA(y_train, order=(3, 1, 2))\n",
    "        elif daten.equals(daten1973):\n",
    "            model = ARIMA(y_train, order=(7, 1, 1))\n",
    "        elif daten.equals(daten1990):\n",
    "            model = ARIMA(y_train, order=(2, 1, 1))\n",
    "        elif daten.equals(daten2006):\n",
    "            model = ARIMA(y_train, order=(2, 1, 5))\n",
    "        else:\n",
    "            pass\n",
    "        model_fit = model.fit()\n",
    "        y_pred = model_fit.forecast(steps=4)\n",
    "\n",
    "    # Arimax\n",
    "    elif methode == \"arimax\":\n",
    "        # ARIMAX-Modell erstellen und anpassen\n",
    "        if daten.equals(daten1950):\n",
    "            model = ARIMA(y_train, order=(10, 4, 1), exog=x_train)\n",
    "        elif daten.equals(daten1973):\n",
    "            model = ARIMA(y_train, order=(3, 3, 1), exog=x_train)\n",
    "        elif daten.equals(daten1990):\n",
    "            model = ARIMA(y_train, order=(1, 1, 2), exog=x_train)\n",
    "        elif daten.equals(daten2006):\n",
    "            model = ARIMA(y_train, order=(2, 1, 1), exog=x_train)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        model_fit = model.fit()\n",
    "        y_pred = model_fit.forecast(steps=4, exog=x_test)\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Bagging einfügen\n",
    "    if ensemble == \"bagging\":\n",
    "        model = BaggingRegressor(model, random_state=42)\n",
    "        # Modell trainieren\n",
    "        model.fit(x_train, y_train)\n",
    "        # Vorhersagen für Testdaten\n",
    "        y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "    # ADA Boosting einfügen\n",
    "    elif ensemble == \"ada_boosting\":\n",
    "        boosting_model = AdaBoostRegressor(base_estimator=model, random_state=42)\n",
    "        # Modell trainieren\n",
    "        boosting_model.fit(x_train, y_train)\n",
    "        # Vorhersagen für Testdaten\n",
    "        y_pred = boosting_model.predict(x_test).flatten()\n",
    "\n",
    "    # Modellprognosen ohne Bagging, Boosting oder ADA Boosting\n",
    "    elif ensemble != \"bagging\" and ensemble != \"boosting\" and ensemble != \"ada_boosting\" and methode != \"nn\" and methode != \"lstm\" and methode != \"arima\" and methode != \"arimax\":\n",
    "        # Modell trainieren\n",
    "        model.fit(x_train, y_train)\n",
    "        # Vorhersagen für Testdaten\n",
    "        y_pred = model.predict(x_test).flatten()\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "# Anzahl der Hauptkomponenten fixieren\n",
    "# daten: daten1950, daten1973, daten1990, daten2006 - Welcher Datensatz genutzt wird\n",
    "# methode: \"lr\", \"pr\", \"sv\", \"knn\", \"arima\", \"arimax\" - Das zu nutzende Modell\n",
    "# normalize: True, False - Ob Daten normalisiert werden\n",
    "def find_best_pca(daten, methode, normalize=True):\n",
    "    # PCA - Dimensionen der Datensätze reduzieren\n",
    "    x = daten.drop([\"co2\"], axis=1)\n",
    "\n",
    "    if normalize:\n",
    "        x = StandardScaler().fit_transform(x)\n",
    "        x = pd.DataFrame(x, columns=daten.columns.drop(\"co2\"))\n",
    "    \n",
    "    if daten.equals(daten1950):\n",
    "        if methode in [\"lr\", \"sv\"]:\n",
    "            n_components = 7\n",
    "        elif methode in [\"pr\", \"knn\", \"arima\"]:\n",
    "            n_components = 2\n",
    "        elif methode in [\"arimax\"]:\n",
    "            n_components = 3\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    elif daten.equals(daten1973):\n",
    "        if methode in [\"lr\", \"arimax\"]:\n",
    "            n_components = 7\n",
    "        elif methode in [\"knn\", \"arima\"]:\n",
    "            n_components = 2\n",
    "        elif methode in [\"pr\"]:\n",
    "            n_components = 5\n",
    "        elif methode in [\"sv\"]:\n",
    "            n_components = 6\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    elif daten.equals(daten1990):\n",
    "        if methode in [\"lr\", \"arimax\"]:\n",
    "            n_components = 5\n",
    "        elif methode in [\"pr\", \"knn\"]:\n",
    "            n_components = 2\n",
    "        elif methode in [\"sv\", \"arima\"]:\n",
    "            n_components = 6\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    elif daten.equals(daten2006):\n",
    "        if methode in [\"lr\", \"sv\", \"knn\"]:\n",
    "            n_components = 2\n",
    "        elif methode in [\"pr\"]:\n",
    "            n_components = 8\n",
    "        elif methode in [\"arimax\", \"arima\"]:\n",
    "            n_components = 6\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # PCA anzahl bestimmen\n",
    "    best_pca = PCA(n_components)\n",
    "    PC = best_pca.fit_transform(x)\n",
    "    principalDF = pd.DataFrame(data=PC)\n",
    "    pca_df = pd.concat([principalDF, daten[['co2']]], axis=1)\n",
    "    \n",
    "    return best_pca, pca_df\n",
    "\n",
    "# Training und Ausgabe der Prognosemodelle / Gebe die Statistiken und Ergebnisse der Prognosen durch\n",
    "# daten: daten1950, daten1973, daten1990, daten2006 - Welcher Datensatz genutzt wird\n",
    "# methode: \"lr\", \"pr\", \"sv\", \"knn\", \"arima\", \"arimax\" - Das zu nutzende Modell\n",
    "# normalize: True, False - Ob Daten normalisiert werden\n",
    "def compare_result2(daten, methode, normalize=True):\n",
    "    # PCA mit niedrigstem MSE finden\n",
    "    best_pca, pca_df = find_best_pca(daten, methode, normalize=True)\n",
    "\n",
    "    # Prognose mit PCA\n",
    "    x_train_pca = pca_df.drop([\"co2\"], axis=1)[:-4]\n",
    "    x_test_pca = pca_df.drop([\"co2\"], axis=1)[-4:]\n",
    "    y_train_pca = pca_df['co2'][:-4]\n",
    "    y_test_pca = pca_df['co2'][-4:]\n",
    "\n",
    "    # Modell mit PCA trainieren\n",
    "    y_pred_pca = modell2(methode, x_train_pca, x_test_pca, y_train_pca, y_test_pca, daten)\n",
    "    \n",
    "    # MSE, RMSE, R², Bias und Varianz berechnen\n",
    "    mae_pca = mean_absolute_error(y_test_pca, y_pred_pca)\n",
    "    rmse_pca = np.sqrt(mean_squared_error(y_test_pca, y_pred_pca))\n",
    "    r2_pca = r2_score(y_test_pca, y_pred_pca)\n",
    "    var_pca = np.var(y_pred_pca)\n",
    "    bias_pca = np.mean(y_pred_pca - y_test_pca)\n",
    "    \n",
    "    # Ausgabe der Metriken\n",
    "    print(\"MAE mit PCA:\", round(mae_pca, 2))\n",
    "    print(\"RMSE mit PCA:\", round(rmse_pca, 2))\n",
    "    print(\"R² mit PCA:\", round(r2_pca, 2))\n",
    "    print(\"Varianz mit PCA:\", round(var_pca, 2))\n",
    "    print(\"Bias mit PCA:\", round(bias_pca, 2))\n",
    "\n",
    "    # Neue Daten für einen Vergleich der Daten vorbereiten\n",
    "    neue_tabelle = pd.DataFrame({'PCA y': y_pred_pca, \"Co2 Emissionen\": y_test_pca})\n",
    "    d = {'jahr': daten[\"jahr\"][-4:], 'Co2 Wert': daten[\"co2\"][-4:], \"PCA y\": daten[\"co2\"][-4:]}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df[\"PCA y\"][-4:] = neue_tabelle[\"PCA y\"]\n",
    "\n",
    "    # Naive Modelle & Mergen der Daten\n",
    "    naiv=pd.DataFrame({\"jahr\": [2019, 2020, 2021, 2022],\"Naiv 1\": [754.408, 754.408, 754.408, 754.408], \"Naiv 2\": [795.557, 800.340, 785.616, 754.408]})\n",
    "    df= pd.merge(df, naiv, on=\"jahr\")\n",
    "    return df\n",
    "\n",
    "# Ausgabe der Einflüsse der einzelnen Variablen auf die PCA\n",
    "# daten: daten1950, daten1973, daten1990, daten2006 - Welcher Datensatz genutzt wird\n",
    "# methode: \"lr\", \"pr\", \"sv\", \"knn\", \"arima\", \"arimax\" - Das zu nutzende Modell\n",
    "# n_components: 2 - 9 - Die Anzahl an Hauptkomponenten festlegen\n",
    "# Contribution: 1, 2 - 1 steht für den Einfluss einer Variable auf alle Hauptkomponenten, 2 steht für den Einfluss auf einzelne Hauptkomponenten\n",
    "# normalize: True, False - Ob Daten normalisiert werden\n",
    "def relevant_variables_per(daten, methode, n_components, Contribution, normalize=True):\n",
    "    pca_ausgaben=1\n",
    "    # PCA - Dimensionen der Datensätze reduzieren\n",
    "    x = daten.drop([\"co2\"], axis=1)\n",
    "\n",
    "    if normalize:\n",
    "        x = StandardScaler().fit_transform(x)\n",
    "\n",
    "    x = pd.DataFrame(x, columns=daten.columns.drop(\"co2\"))\n",
    "\n",
    "    # Liste für MAE-Werte\n",
    "    mae_values = []\n",
    "\n",
    "    # PCA durchführen\n",
    "    pca = PCA(n_components=n_components)\n",
    "    PC = pca.fit_transform(x)\n",
    "    principalDF = pd.DataFrame(data=PC, columns=[f'pc{i}' for i in range(1, n_components + 1)])\n",
    "    pca_df = pd.concat([principalDF, daten[['co2']]], axis=1)\n",
    "\n",
    "    # Festlegen des Trainings- und Testdatenbereichs\n",
    "    x_train = pca_df.drop([\"co2\"], axis=1)[:-4]\n",
    "    x_test = pca_df.drop([\"co2\"], axis=1)[-4:]\n",
    "    y_train = pca_df['co2'][:-4]\n",
    "    y_test = pca_df['co2'][-4:]\n",
    "\n",
    "    # Ausgewählte Modellprognose\n",
    "    y_pred = modell2(methode, x_train, x_test, y_train, y_test, daten)\n",
    "\n",
    "    # Berechnung des Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mae_values.append(mae)\n",
    "\n",
    "    # Ausgabe des MAE-Werts\n",
    "    print(f\"Anzahl der PCA-Komponenten: {n_components}, MAE: {mae}\")\n",
    "\n",
    "    if Contribution == 1:\n",
    "        # 1. Einfluss der Variablen über alle Hauptkomponenten\n",
    "        # Variablen mit höchstem Beitrag zu den ausgewählten PCA-Komponenten finden\n",
    "        # Mithilfe von Chatgpt generiert\n",
    "        selected_components = [f'pc{i}' for i in range(1, n_components + 1)]\n",
    "        variable_contributions = pd.DataFrame(\n",
    "            data=pca.components_,\n",
    "            columns=x.columns,\n",
    "            index=selected_components\n",
    "        ).abs()\n",
    "\n",
    "        # Prozentsatz der Wichtigkeit der Variablen über alle ausgewählten Komponenten berechnen\n",
    "        variable_importance_percentage = (variable_contributions / variable_contributions.sum(axis=1).to_numpy().reshape(-1, 1)) * 100\n",
    "\n",
    "        # Gesamtprozentsatz der Wichtigkeit jeder Variable berechnen (über alle Komponenten)\n",
    "        variable_total_importance_percentage = variable_importance_percentage.sum()/n_components\n",
    "\n",
    "        # Sortieren der Variablen nach Gesamtprozentsatz der Wichtigkeit\n",
    "        sorted_variables = variable_total_importance_percentage.sort_values(ascending=False)\n",
    "\n",
    "        # Ausgabe der Variablen und ihrer prozentualen Wichtigkeit\n",
    "        print(\"Wichtigste Variablen über alle ausgewählten PCA-Komponenten:\")\n",
    "        for variable, importance in sorted_variables.iteritems():\n",
    "            print(f\"Variable: {variable}, Wichtigkeit: {importance:.2f}%\")\n",
    "\n",
    "    elif Contribution == 2:\n",
    "        # 2. Einfluss der Variablen über einzelne Hauptkomponenten\n",
    "        # Variablen mit höchstem Beitrag zu den ausgewählten PCA-Komponenten finden\n",
    "        # Mithilfe von Chatgpt generiert\n",
    "        selected_components = [f'pc{i}' for i in range(1, n_components + 1)]\n",
    "        highest_contributions = pd.DataFrame(\n",
    "            data=pca.components_,\n",
    "            columns=x.columns,\n",
    "            index=selected_components\n",
    "        )\n",
    "\n",
    "        # Prozentsatz der Wichtigkeit der Variablen über alle ausgewählten Komponenten berechnen\n",
    "        variable_importance_percentage = (highest_contributions.abs() / highest_contributions.abs().sum(axis=1).values[:, None]) * 100\n",
    "\n",
    "        # Ausgabe der prozentualen Gewichte der Variablen für jede Hauptkomponente\n",
    "        print(f\"Prozentuale Gewichte der Variablen für {n_components} ausgewählte PCA-Komponenten:\")\n",
    "        for component in selected_components:\n",
    "            print(f\"PCA-Komponente {component}\")\n",
    "            print(variable_importance_percentage.loc[component].sort_values(ascending=False))\n",
    "            print()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# PLotten der Modelle LR, PR, SVR, Arima und Arimax zum direkten Vergleich\n",
    "# daten: daten1950, daten1973, daten1990, daten2006 - Welcher Datensatz genutzt wird\n",
    "# Die Daten sind sehr redundant, da mir in anderer Form mit Schleifen nicht alle Lineplots korrekt angezeigt wurden\n",
    "def plotten(daten):\n",
    "    # Erzeugen von Werten in den Prognosejahren 2019-2022\n",
    "    lr=compare_result2(daten, \"lr\", normalize=True)\n",
    "    pr=compare_result2(daten, \"pr\", normalize=True)\n",
    "    sv=compare_result2(daten, \"sv\", normalize=True)\n",
    "    knn=compare_result2(daten, \"knn\", normalize=True)\n",
    "    ama=compare_result2(daten, \"arima\", normalize=True)\n",
    "    amax=compare_result2(daten, \"arimax\", normalize=True)\n",
    "    \n",
    "    # Umbenennung der Spalten in den Prognosespalten\n",
    "    lr= lr.rename(columns={'PCA y': 'LR'})\n",
    "    pr= pr.rename(columns={'PCA y': 'PR'})\n",
    "    sv= sv.rename(columns={'PCA y': 'SVR'})\n",
    "    knn= knn.rename(columns={'PCA y': 'KNN'})\n",
    "    ama= ama.rename(columns={'PCA y': 'Arima'})\n",
    "    amax= amax.rename(columns={'PCA y': 'Arimax'})\n",
    "    \n",
    "    # Mergen der Daten zu einem Dataframe\n",
    "    df = pd.merge(lr, pr, on=[\"jahr\", \"Co2 Wert\", \"Naiv 1\", \"Naiv 2\"]) \\\n",
    "    .merge(sv, on=[\"jahr\", \"Co2 Wert\", \"Naiv 1\", \"Naiv 2\"]) \\\n",
    "    .merge(ama, on=[\"jahr\", \"Co2 Wert\", \"Naiv 1\", \"Naiv 2\"]) \\\n",
    "    .merge(amax, on=[\"jahr\", \"Co2 Wert\", \"Naiv 1\", \"Naiv 2\"]) \\\n",
    "    .merge(knn, on=[\"jahr\", \"Co2 Wert\", \"Naiv 1\", \"Naiv 2\"])\n",
    "    \n",
    "    # Identische Datenströme erzeugen\n",
    "    co=daten[[\"co2\", \"jahr\"]][-8:-4]\n",
    "    co = co.assign(LR=co[\"co2\"])\n",
    "    co = co.assign(PR=co[\"co2\"])\n",
    "    co = co.assign(SVR=co[\"co2\"])\n",
    "    co = co.assign(KNN=co[\"co2\"])\n",
    "    co = co.assign(Arima=co[\"co2\"])\n",
    "    co = co.assign(Arimax=co[\"co2\"])\n",
    "    co = co.assign(Naiv1=co[\"co2\"])\n",
    "    co = co.assign(Naiv2=co[\"co2\"])\n",
    "    co = co.rename(columns={'co2': 'Co2 Wert', \"Naiv1\": \"Naiv 1\", \"Naiv2\": \"Naiv 2\"})\n",
    "\n",
    "    # Vermischen der Daten bis 2018\n",
    "    df=pd.concat([co, df], ignore_index=True)\n",
    "    \n",
    "    # Plot vergrößern\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Daten für den Plot\n",
    "    sns.lineplot(data=df, x=\"jahr\", y=\"Naiv 1\", linewidth=1, label=\"Naiv 1\")\n",
    "    sns.lineplot(data=df, x=\"jahr\", y=\"Naiv 2\", linewidth=1, label=\"Naiv 2\")\n",
    "    sns.lineplot(data=df, x=\"jahr\", y=\"Co2 Wert\", linewidth=1, linestyle='dashed', label=\"Co2\")\n",
    "    sns.lineplot(data=df, x=\"jahr\", y=\"LR\", linewidth=1, label=\"LR\")\n",
    "    sns.lineplot(data=df, x=\"jahr\", y=\"PR\", linewidth=1, label=\"PR\")\n",
    "    sns.lineplot(data=df, x=\"jahr\", y=\"KNN\", linewidth=1, label=\"KNN\")\n",
    "    sns.lineplot(data=df, x=\"jahr\", y=\"SVR\", linewidth=1, label=\"SVR\")\n",
    "    sns.lineplot(data=df, x=\"jahr\", y=\"Arima\", linewidth=1, label=\"Arima\")\n",
    "    sns.lineplot(data=df, x=\"jahr\", y=\"Arimax\", linewidth=1, label=\"Arimax\")\n",
    "\n",
    "    # Plot erstellen\n",
    "    plt.xlabel('Jahr')\n",
    "    plt.ylabel('Co2 (in Mio. Tonnen)')\n",
    "    plt.title('Vergleich der Prognosemodelle')\n",
    "    plt.legend()\n",
    "\n",
    "    # Anzeigen des Plots\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
